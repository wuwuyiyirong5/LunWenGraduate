%%---------------------------------------------------------------------------%%
%%------------ 第三章：矩阵根的~Euler~法 ------------------------------------%%
%%---------------------------------------------------------------------------%%


\chapter{计算矩阵主~$p$~次根的~Euler~法}
\label{chapter:EM_MatrixRoot}


\section{Introduction}

Let integer $p \geq 2$. A matrix $X \in \CS^{n \times n}$ is called
a $p$th root of a matrix $A \in \CS^{n \times n}$ if $X^p = A$. If
$A$ has no eigenvalues on $\RS^-$, the closed negative real axis,
and all zero eigenvalues of $A$ are semisimple, there exists a
unique principal $p$th root of $A$, denoted by $A^{1/p}$
\cite{Lin2010}. Eigenvalues of $A^{1/p}$ have argument less in
modulus than $\pi/p$. An application of $p$th root is in the
computation of the matrix logarithm through the relation $\log A = p
\log A^{1/p}$, where $p$ is chosen so that $A^{1/p}$ can be well
approximated by a polynomial or rational function. One can see
\cite{Higham2008} or the recent survey paper \cite{HighamAlMohy2010}
for more applications.

There are two various ways to deal with the matrix $p$th root. The
first way is to use the direct Schur decomposition of $A$, say
$Q^*AQ = R$, to solve the equation $Y^p = R$, instead of the
equation $X^p = A$, by appropriate recurrences on the elements of
$Y$, see for example
\cite{Smith2003,GrecoIannazzo2010,Iannazzo2013}. The second way is
to use the matrix iterations which converge to the principal $p$th
root of $A$, see for example
\cite{BiniHighamMeini2005,GuoHigham2006,Guo2010,Iannazzo2006,Iannazzo2008,
Laszkiewicz2009,Lin2010,Gomilko2012,Zietak2013}.

In this paper, we are interested in the method of matrix iterations.
They are mainly deduced from iterative method for solving nonlinear
equation. When Newton's method is applied on the matrix equation
\begin{equation}
\label{eq:f(X)=0} f(X) = X^p - A = 0,
\end{equation}
Iannazzo shown that the matrix sequence $\{X_k\}$ generated by
Newton's method starting from the identity matrix converges to the
principal $p$th root $A^{1/p}$ if all the eigenvalues of $A$ are in
the set $\MCD = \{z \in \CS: |z| \leq 1, \Real z > 0\}$ or in the
set $\mathcal {E} = \{z \in \CS: 0 < |z| \leq 2, |\arg(z)| <
\pi/4\}$ in \cite{Iannazzo2006} and \cite{Iannazzo2008},
respectively. Subsequently, Guo obtained in \cite{Guo2010} that the
matrix sequence converges to the principal $p$th root of $A$ also
when the eigenvalues of $A$ lie in the set $\mathcal {F} = \{z \in
\CS: |z - 1| \leq 1\}$.


The convergence study for Halley's method applied to
(\ref{eq:f(X)=0}) is studied in \cite{Iannazzo2008} where Iannazzo
proved that the matrix sequence $\{X_k\}$ generated by Halley's
method starting from the identity matrix converges to the principal
$p$th root $A^{1/p}$ for each $A$ having eigenvalues in the set
$\mathcal {G} = \{z \in \CS: \Real z > 0\}$. Guo further proved the
order of convergence is quadratic and cubic for Newton iteration and
Halley iteration in \cite{Guo2010}, respectively.


Methods based on Pad\'{e} approximation (Pad\'{e} family of
iterations and dual Pad\'{e} family of iterations) for computing the
principal $p$th root are also investigated by several authors, see
for example \cite{Laszkiewicz2009, Gomilko2012,Zietak2013}. Note
that Newton's method and Halley's method are special cases as dual
Pad\'{e} family of iterations. Meanwhile, Halley's method is also a
special case of Pad\'{e} family of iterations, see \cite{Zietak2013}
for more details.


As you known, Euler's method is also a famous iteration for solving
nonlinear operator equation.  Euler's method for (\ref{eq:f(X)=0})
in classical form
\begin{equation*}
X_{k + 1} = X_k - [\IO + L_f(X_k)] \inv{f'(X_k)} f(X_k), \ \ k = 0,
1, 2, \ldots,
\end{equation*}
where $\IO$ denotes the identity operator and operator $L_f(X) =
\frac{1}{2} \inv{f'(X)} f''(X) \inv{f'(X)} f(X)$, can be rewritten
as follows:
\begin{equation}
\label{it:EM} X_{k + 1} = E(X_k), \ \ k =0, 1, 2,  \ldots,
\end{equation}
provided $X_0$ commutes with $A$ and $X_k$ is nonsingular for any $k
\geq 0$, where
\begin{equation}
\label{it:EM_fun} E(X) = \displaystyle\frac{1}{2p^2} X \left[(2 p^2
- 3 p + 1)\I + 2(2 p - 1)AX^{-p} - (p - 1)\left(
AX^{-p}\right)^2\right]
\end{equation}
for nonsingular matrix $X \in \CS^{n \times n}$.


A natural question is, how it should be when we apply this method to
solve the principal $p$th root of $A$? It becomes the main goal of
this paper. It is worth noting that, Newton's method and Euler's
method (\ref{it:EM}) are  special cases as the family of
Schr\"{o}der iteration which has recently been studied for the $p$th
root of complex numbers, see for example
\cite{Cardoso2011,Cardoso2011a}.


In the case of scalar Newton iteration  the only existing fixed
points are the $p$th roots of $\lambda$. For scalar Halley
iteration, it has not only the $p$th roots of $\lambda$ as the fixed
points, but has the extra fixed point $z = 0$. While for Euler's
method (\ref{it:EM}), the extra fixed points are $z = 0$ as well as
the $p$th roots of $(p-1)\lambda/(3p-1)$. Based on the above
intrinsic properties, we cannot get the convergence region of
Halley's method obtained in \cite{Iannazzo2008} for Euler's method
(\ref{it:EM}). However, we determine a certain convergence region
for Euler's method (\ref{it:EM}) including that one for Newton's
method given in \cite{Guo2010}. Furthermore, we also analyze the
robust of Euler's method (\ref{it:EM}) and give a modification based
on Schur decomposition which has been applied to Newton and Halley's
methods in \cite{GuoHigham2006} and \cite{Iannazzo2008},
respectively. Numerical experiments illustrate that the modified
algorithm has good numerical properties  as the existing algorithms
and confirm that the Euler method is a good choice for solving the
matrix $p$th root.





This paper is organized as follows. In Section 2, we state our main
result on the convergence of Euler's method (\ref{it:EM}) for
computing the matrix $p$th root. And then we prove this result in
Section 3. In Section 4, we describe our general algorithm and
discuss some related computational issues. Finally in Section 5 we
present some numerical experiments and compare our method with
others existing methods. These results confirm the numerical
stability the overall good performance of the new algorithm.




\section{Convergence results for Euler's method}


Throughout the whole paper, we always suppose that the integer $p
\geq 2$. To state the convergence results for Euler's method
(\ref{it:EM}), we introduce some notations. Let
\begin{equation*}
\label{fun:u(z)} u(z) := \frac{1}{1 - \frac{1}{p}z -
\frac{1}{2}\left(\frac{1}{p} - \frac{1}{p^2}\right)z^2}, \ \ \ z \in
\MCD_0
\end{equation*}
and
\begin{equation}
\label{fun:phi(z)} \phi(z) := 1 - u(z)^p(1 - z), \ \ \  z \in
\MCD_0,
\end{equation}
where
\begin{equation}
\label{set:D0} \MCD_0 := \left\{z \in \CS: |z| < \frac{p}{p -
1}(\sqrt{2p - 1} - 1)\right\}.
\end{equation}


Define
\begin{equation}
\label{set:R} \MCR := 1 - \overline{\MCD}_1 \bigcup \left(\MCD_0
\bigcap \MCD_2\right) := \left\{1-z: z \in \overline{\MCD}_1 \bigcup
\left(\MCD_0 \bigcap \MCD_2\right)\right\}
\end{equation}
and
\begin{equation}
\label{set:R_hat} \widehat{\MCR} := 1 - \mathcal {D}_1 \bigcup
\left(\mathcal {D}_0 \bigcap \mathcal {D}_2\right) := \left\{1-z: z
\in \mathcal {D}_1 \bigcup \left(\mathcal {D}_0 \bigcap \mathcal
{D}_2\right)\right\},
\end{equation}
where $\MCD_0$ is defined in (\ref{set:D0}), $\overline{\MCD}_1$ is
the closure of $\MCD_1$ defined by
\begin{equation}
\label{set:D1} \MCD_1  := \{z \in \CS: |z| < 1\},
\end{equation}
and
\begin{equation}
\label{set:D2} \MCD_2  := \left\{z \in \CS: \sup_{m \geq 3}
\left\{\frac{|S_m(z)|}{|z|}\right\} \cdot \frac{|z| +|\phi(z)|}{|z|
- |\phi(z)|} < 1 \right\},
\end{equation}
where $S_m(z) = \sum\limits_{j = 3}^{m} c_j z^{j}$, $c_j =
\phi^{(j)}(0)/j!, j = 3, 4, \ldots$, and $\phi$ is defined in
(\ref{fun:phi(z)}). That is,
$$
\MCR = \{z\in\CS: 1-z \in \overline{\MCD}_1\} \bigcup \{z\in\CS: 1-z
\in \MCD_0 \bigcap \MCD_2\}.
$$


We have


\begin{theorem}
\label{th:MatrixEMCon} If all eigenvalues of $A \in \CS^{n \times
n}$ are in $\MCR$ defined in $(\ref{set:R})$ and all zero
eigenvalues of $A$ are semisimple, then the matrix sequence
$\{X_k\}$ generated by Euler's method $(\ref{it:EM})$ starting from
$X_0 = \I$ converges to the principal $p$th root $A^{1/p}$.
Moreover, if all eigenvalues are in $\widehat{\MCR}\backslash\{0\}$
defined in $(\ref{set:R_hat})$, then the convergence is cubic.
\end{theorem}



\begin{figure}[p!]
\centering
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p25m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p25m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p25m500.eps}}\\
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p100m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p100m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p100m500.eps}}\\
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p400m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p400m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_EM_ConvReg_p400m500.eps}}\\
\caption{The approximating regions of $\mathcal {R}$ defined in
(\ref{set:R}) for $p = 25, 100, 400$ and $m = 20, 100, 500$, where
the red and blue regions denote the set $\{z\in\CS: 1-z \in
\overline{\MCD}_1\}$ and  $\{z\in\CS: 1-z \in \MCD_0 \bigcap
\MCD_2\}$, respectively. }\label{fig:EM_ConvReg}
\end{figure}


\begin{remark}
Figure \ref{fig:EM_ConvReg} plots the approximating region of $\MCR$
with nine cases in the complex plane, where the red and blue regions
denote the sets $\{z\in\CS: 1-z \in \overline{\MCD}_1\}$ and
$\{z\in\CS: 1 - z \in \MCD_0 \bigcap \MCD_2\}$, respectively. Since
$$
\sum_{j = 1}^m c_j^1(1-z)^{j-1} \to \frac{\phi_1(1-z)}{1-z}, \ \ \ n
\to \infty,
$$
for each $z \in \{z\in\CS: 1-z \in \MCD_0\}$ by Lemma
\ref{lem:RatFunTayExp} in Section 3.1, we can see from Figure
\ref{fig:EM_ConvReg} that approximating regions ((a)-(c), (d)-(f),
(g)-(i)) are almost the same for a fixed $p$ when $m=20, 100, 500$,
respectively. To this end, it suffices to choose $m=20$ in practical
numerical computation.
\end{remark}

\begin{remark}
The set $\MCD_0$ defined in (\ref{set:D0}) can be enlarged as
$$
\left\{z \in \CS: \frac{1}{p}|z|\left|1 + \frac{1}{2}(1 -
\frac{1}{p})z\right|<1 \right\}.
$$
However, this improvement does not affect the convergence region
$\MCR$ defined in (\ref{set:R}). So, for convenience, we still use
$\MCD_0$ in our convergence analysis in Section 3.
\end{remark}


\begin{remark}
In practice,  it is not feasible to check whether a eigenvalue
$\lambda$ belongs to $\MCR$. This is due to the computational cost
of (\ref{set:D2}) may be large even if we only choose $m=20$. Thus,
based on the observation from Figure \ref{fig:EM_ConvReg}, we now
give a new convergence region which allows us to determine easily
whether a eigenvalue belongs to it. Define
\begin{equation}
\label{set:R_pra} \MCR_{\text{E}} := \MCD_3 \bigcup \MCD_4,
\end{equation}
where
\begin{align*}
\MCD_3 & := \{z \in \CS: 1-z \in \overline{\MCD}_1 \},\\
\MCD_4 & := \left\{z \in \CS: |\arg(z)| < \frac{\pi}{4}, |1-z| <
\frac{31}{24}\right\}.
\end{align*}
In view of $p(\sqrt{2p - 1} - 1)/(p-1) > 31/24$ for any $p \geq 2$,
one has that $\MCD_4 \subset \MCD_0$. In Figure
\ref{fig:EM_PraConvReg_p100n20}, for $p=100, m=20$, we present the
the regions $\MCR$ and $\MCR_{\text{E}}$ defined in (\ref{set:R})
and (\ref{set:R_pra}), respectively, of Euler's method (\ref{it:EM})
for computing the $p$th root of a matrix. We can observe that the
new region $\MCR_{\text{E}}$ is acceptable approximation to $\MCR$.
So instead of using $\MCR$, in Sections 4 and 5, we will use
$\MCR_{\text{E}}$ in our algorithm and numerical experiments.
\end{remark}


\begin{figure}[t!]
\begin{center}
\scalebox{0.75}[0.75]{\includegraphics{fig_EM_PraConvReg_p100m20.eps}}\\
\end{center}
\caption{For $p=100, m=20$, the actual convergence regions $\MCR$
defined in (\ref{set:R}) (the union of the red and blue parts) and
the approximate convergence regions $\MCR_{\text{E}}$ defined in
(\ref{set:R_pra}) of Euler's method (the yellow parts).}
\label{fig:EM_PraConvReg_p100n20}
\end{figure}



\section{Proof of Theorem \ref{th:MatrixEMCon}}


In this section, we will prove Theorem \ref{th:MatrixEMCon}, the
results on convergence and convergence order for Euler's method
(\ref{it:EM}).


\subsection{Technical lemmas}


The following lemma is taken from \cite[Theorem 3.2]{Cardoso2011}.


\begin{lemma}[\cite{Cardoso2011}]
\label{lem:RatFunTayExp} The Maclaurin series of the function
$\phi(z)$ defined by $(\ref{fun:phi(z)})$ has the form
\begin{equation}
\label{eq:phi(t)_series_form} \phi(z) = \sum_{j = 0}^\infty c_j z^j,
\ \ \ z \in \MCD_0,
\end{equation}
where $\MCD_0$ is defined in $(\ref{set:D0})$, and the coefficients
$c_j = \phi^{(j)}(0)/j!$ satisfying $c_j
> 0$ for all $j \geq 3$ and $\sum\limits_{j = 3}^\infty c_j = 1$.
\end{lemma}



For a complex number $\lambda \in \CS$, let
\begin{equation}
\label{fun:f(z)} f(z) := z^p - \lambda, \ \ \ z \in \CS
\end{equation}
and
\begin{equation}
\label{fun:r(z)} r(z,\lambda) := 1 - \lambda z^{-p}, \ \ \ z \in
\CS\backslash\{0\}.
\end{equation}


\begin{lemma}
\label{lem:r(E(z))_r(z)} Let $r(z,\lambda)$ be defined in
$(\ref{fun:r(z)})$. If $r(z,\lambda) \in \mathcal
{D}_0\backslash\{1\}$ for some $z \in \CS\backslash\{0\}$, where
$\MCD_0$ is defined in $(\ref{set:D0})$, then $E(z)$ generated by
the scalar case of $(\ref{it:EM_fun})$ for $f(z)$ defined in
$(\ref{fun:f(z)})$ exists and
\begin{equation}
\label{eq:r(E(z))_1} r(E(z),\lambda) = \phi(r(z,\lambda)),
\end{equation}
where $\phi$ is defined by $(\ref{fun:phi(z)})$. Moreover, we have
\begin{numcases}{}
|r(E(z),\lambda)| < |r(z,\lambda)|^3, &
if \ $r(z,\lambda) \in \overline{\MCD}_1\backslash\{0,1\}$,\ \ \  \label{ineq:abs_r(E(z))_1}\\
|r(E(z),\lambda)| \leq \sup_{m \geq
3}\left\{\left|\frac{S_m(u)}{u^3}\right|\right\}\cdot \frac{|u| +
|r(z,\lambda)|}{|u| - |r(z,\lambda)|} \cdot |r(z,\lambda)|^3,  & if
\  $r(z,\lambda) \in \mathcal {D}_0\backslash\{1\}$,
\label{ineq:abs_r(E(z))_2}
\end{numcases}
where $\overline{\MCD}_1$ is the closure of $\MCD_1$ defined in
$(\ref{set:D1})$, and $S_m(u)$ is the $m$th partial sum of the
series $(\ref{eq:phi(t)_series_form})$ for each $u \in \mathcal
{D}_0$ satisfying $|u| > |r(z,\lambda)|$, $m \geq 3$.
\end{lemma}




\begin{proof}
For any $z \in \CS\backslash\{0\}$, $E(z)$ exists by
(\ref{it:EM_fun}). Furthermore, when
 $r(z,\lambda) \in \MCD_0\backslash\{0,1\}$, since
\begin{eqnarray*}
\lefteqn{\frac{1}{2p^2}  \left|(2 p^2 - 3 p + 1) + 2(2 p - 1)
\lambda z^{-p} - (p - 1)(\lambda z^{-p})^2\right|} \nonumber\\
& = &  \left|\frac{2p^2 - 3p + 1}{2p^2} + \frac{2p -
1}{p^2}(1 - r(z,\lambda)) - \frac{p - 1}{2p^2}(1 - r(z,\lambda))^2\right| \nonumber \\
& = &  \left|1 - \frac{1}{p} r(z,\lambda) -
\frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)r^2(z,\lambda)\right| \nonumber \\
& > & 1 - \left[\frac{1}{p} |r(z,\lambda)| +
\frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)|r(z,\lambda)|^2\right]\\
&>& 0,\label{eq:zk+1}
\end{eqnarray*}
we know
\begin{equation*}
\label{eq:E(z)} E(z) = \frac{1}{2p^2} z \left[(2 p^2 - 3 p + 1) +
2(2 p - 1) \lambda z^{-p} - (p - 1)(\lambda z^{-p})^2\right] \neq 0
\end{equation*}
and
\begin{equation}
\label{eq:r(E(z))} r(E(z),\lambda) = 1 - \left[1 - \frac{1}{p}
r(z,\lambda) - \frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)r^2(z,\lambda)\right]^{-p}\cdot(1 -
r(z,\lambda)) = \phi(r(z,\lambda)),
\end{equation}
i.e., (\ref{eq:r(E(z))_1}) holds. (\ref{eq:r(E(z))}) shows that
$r(E(z),\lambda) \neq 1$ when $r(z,\lambda) \neq 1$.

If $r(z,\lambda) \in \overline{\MCD}_1\backslash\{0,1\} \subset
\MCD_0$, then, by (\ref{eq:phi(t)_series_form}) in Lemma
\ref{lem:RatFunTayExp} and (\ref{eq:r(E(z))}), one has that
\begin{align}
|r(E(z),\lambda)|  = |\phi(r(z,\lambda))| & = |r(z,\lambda)|^3 \cdot
\left| \sum_{j =
3}^\infty c_j r^{j-3}(z,\lambda) \right| \nonumber\\
& \leq |r(z,\lambda)|^3 \left[|c_3 + c_4 r(z,\lambda)| + \sum_{j =
5}^\infty c_j |r(z,\lambda)|^{j-3}
\right] \nonumber\\
& < |r(z,\lambda)|^3 \sum_{j = 3}^\infty c_j = |r(z,\lambda)|^3 \nonumber\\
& \leq |r(z,\lambda)| \label{ineq:abs_r(E(z))}
\end{align}
from Lemma \ref{lem:RatFunTayExp} and the fact that $|c_3 + c_4 w| <
c_3 + c_4$ for all $w \in \overline{\MCD}_1\backslash\{1\}$. Thus,
(\ref{ineq:abs_r(E(z))_1}) is proved.

If $r(z,\lambda) \in \mathcal {D}_0\backslash\{1\}$, based on
(\ref{eq:phi(t)_series_form}) in Lemma \ref{lem:RatFunTayExp} and
(\ref{eq:r(E(z))}) again, we have
\begin{align*}
r(E(z),\lambda)  = \phi(r(z,\lambda)) = \sum_{j=3}^\infty c_j
r^j(z,\lambda) = \left[\sum_{j=3}^\infty c_j u^{j-3}
\left(\frac{r(z,\lambda)}{u}\right)^{j-3}\right] \cdot
r^3(z,\lambda)
\end{align*}
holds for any $u \in \CS\backslash\{0\}$. Since, for any $m \geq 3$,
\begin{align*}
\sum_{j=3}^m c_j u^{j-3} \left(\frac{r(z,\lambda)}{u}\right)^{j-3} &
= \sum_{j=3}^{m-1} \left(\sum_{\ell=3}^j c_\ell
u^{\ell-3}\right)\left(1 -
\frac{r(z,\lambda)}{u}\right)\left(\frac{r(z,\lambda)}{u}\right)^{j-3}\\
& \ \ \ + \left(\sum_{\ell = 3}^m c_\ell u^{\ell-3}\right)\cdot
\left(\frac{r(z,\lambda)}{u}\right)^{m-3}
\end{align*}
by Abel transformation, we have
\begin{align*}
\left|\sum_{j=3}^m c_j u^{j-3}
\left(\frac{r(z,\lambda)}{u}\right)^{j-3}\right| & \leq \sup_{3\leq
j \leq m-1} \left\{\left|\sum_{\ell=3}^j c_\ell
u^{\ell-3}\right|\right\} \cdot \left|1 -
\frac{r(z,\lambda)}{u}\right| \cdot \sum_{j=3}^{m-1}
\left|\frac{r(z,\lambda)}{u}\right|^{j-3}\\
& \ \ \ + \left|\frac{S_m(u)}{u^3} \right|\cdot
\left|\frac{r(z,\lambda)}{u}\right|^{m-3}, \ \ \ m >3.
\end{align*}
Then, letting $m \to \infty$ in the above inequality, it follows
that
\begin{align*}
|r(E(z),\lambda)|  & \leq \sup_{m \geq 3} \left\{\left|\sum_{j=3}^m
c_j u^{j-3}\right|\right\} \cdot \left|1 -
\frac{r(z,\lambda)}{u}\right| \cdot
\sum_{m=3}^\infty \left|\frac{r(z,\lambda)}{u}\right|^{m-3}\cdot |r(z,\lambda)|^3 \\
& = \sup_{m \geq 3} \left\{\left|\frac{S_m(u)}{u^3}\right|\right\}
\cdot \frac{\left|1 - \frac{r(z,\lambda)}{u}\right|}{1 -
\left|\frac{r(z,\lambda)}{u}\right|} \cdot |r(z,\lambda)|^3 \\
& = \sup_{m \geq 3} \left\{\left|\frac{S_m(u)}{u^3}\right|\right\}
\cdot \frac{|u| + |r(z,\lambda)|}{|u| - \left|r(z,\lambda)\right|}
\cdot |r(z,\lambda)|^3
\end{align*}
for any $r(z,\lambda) \in \mathcal {D}_0\backslash\{1\}$ and any $u
\in \mathcal {D}_0$ subject to $|u|
> |r(z,\lambda)|$, which verifies (\ref{ineq:abs_r(E(z))_2}). The
proof is completed.
\end{proof}



Based on Lemma \ref{lem:r(E(z))_r(z)}, we have the following lemma
which is more refined than the one obtained in \cite[Corollary
3.1]{Cardoso2011}.


\begin{lemma}
\label{lem:r(z)_convergence1} Let $r(z,\lambda)$ be defined in
$(\ref{fun:r(z)})$. If $r(z_0,\lambda) \in
\overline{\MCD}_1\backslash\{0,1\}$ for some $z_0 \in
\CS\backslash\{0\}$, then the sequence $\{z_k\}$ starting from $z_0$
generated by the scalar case of $(\ref{it:EM})$ for solving
$(\ref{fun:f(z)})$ exists,
\begin{equation}
\label{ineq:abs_r(zk)_1} |r(z_k,\lambda)| \leq q_1^{3^{k-1}}(z_0), \
\ \ k = 1, 2, \ldots,
\end{equation}
where
\begin{equation}
\label{cons:q1(z0)} q_1(z_0) = q_1(z_0,\lambda) :=
\left|\sum_{j=3}^\infty c_j r^{j-1}(z_0,\lambda)\right| < 1,
\end{equation}
and so $|r(z_k,\lambda)| \to 0$ with order $3$ as $k \to \infty$.
\end{lemma}

\begin{proof}
For $z_0$ chosen, $q_1(z_0) < 1$ follows from the same arguments
used in (\ref{ineq:abs_r(E(z))}). By (\ref{ineq:abs_r(E(z))_1}) in
Lemma \ref{lem:r(E(z))_r(z)}, $z_1 = E(z_0)$ exists and
$$
|r(z_1,\lambda)| = q_1(z_0)|r(z_0,\lambda)| \leq q_1(z_0) < 1.
$$
Suppose that $z_k$ exists and (\ref{ineq:abs_r(zk)_1}) holds for
some $k \geq 1$, then by Lemma \ref{lem:r(E(z))_r(z)} again,
$z_{k+1} = E(z_k)$ exists and
\begin{equation*}
|r(z_{k+1},\lambda)| < |r(z_{k},\lambda)|^3  \leq
\left[q_1^{3^{k-1}}(z_0)\right]^3 = q_1^{3^{k}}(z_0).
\end{equation*}
Thus, (\ref{ineq:abs_r(zk)_1}) holds for $k+1$. By induction,
$\{z_k\}$ exists and (\ref{ineq:abs_r(zk)_1}) holds. Furthermore,
$r(z_k,\lambda) \to 0$ with order $3$ as $k \to \infty$, which
completes the proof.
\end{proof}



The following lemma says that, besides in $
\overline{\MCD}_1\backslash\{0,1\}$, it also guarantees
$r(z_k,\lambda)$ converges cubically to 0 as $k \to \infty$ when
$r(z_0,\lambda) \in \MCD_2 \bigcap \MCD_0\backslash\{1\}$ for some
$z_0 \in \CS\backslash\{0\}$.

\begin{lemma}
\label{lem:r(z)_convergence2} 我们   Let $r(z,\lambda)$ be defined
in $(\ref{fun:r(z)})$. For any $z_0 \in \CS\backslash\{0\}$
satisfying $r(z_0,\lambda) \in \MCD_2 \bigcap \MCD_0\backslash\{1\}$
and
\begin{equation}
\label{cons:q2(z0)} q_2(z_0) = q_2(z_0,\lambda) := \sqrt{\sup_{m
\geq 3}
\left\{\frac{|S_m(r(z_0,\lambda))|}{|r(z_0,\lambda)|}\right\} \cdot
\frac{|r(z_0,\lambda)|
+|\phi(r(z_0,\lambda))|}{\big||r(z_0,\lambda)| -
|\phi(r(z_0,\lambda))|\big|}} < 1,
\end{equation}
where $\MCD_2$ is defined in $(\ref{set:D2})$, the sequence
$\{z_k\}$ generated by the scalar form of $(\ref{it:EM})$ starting
from $z_0$ for solving $(\ref{fun:f(z)})$ exists,
\begin{equation}
\label{ineq:abs_r(zk)_2} |r(z_k,\lambda)| \leq q_2^{3^k-1}(z_0)
\cdot |r(z_0,\lambda)|, \ \ \ k = 0, 1, \ldots,
\end{equation}
and so $|r(z_k,\lambda)| \to 0$ with order $3$ as $k \to \infty$.
\end{lemma}

\begin{proof}
For $z_0$ chosen, we have $r(z_0) \in \mathcal
{D}_0\backslash\{1\}$. So, $z_1 = E(z_0)$ exists and $z_1 \neq 0$ by
Lemma \ref{lem:r(E(z))_r(z)}. Recall that $S_m(r(z_0)) \to
\phi(r(z_0))$ as $k\to\infty$ implies
$$
\left|\frac{\phi(r(z_0))}{r(z_0)}\right| \leq \sup_{m\geq3}
\left\{\left|\frac{S_m(r(z_0))}{r(z_0)}\right|\right\} <
q_2^2(z_0)<1,
$$
by (\ref{cons:q2(z0)}), we have
$$
|r(z_1)| = |\phi(r(z_0))| =
\left|\frac{\phi(r(z_0))}{r(z_0)}r(z_0)\right| <
q_2^2(z_0)\cdot|r(z_0)|,
$$
and (\ref{ineq:abs_r(zk)_2}) holds for $k=1$.  Assume $z_0, z_1,
\ldots, z_k$ exist and satisfy (\ref{ineq:abs_r(zk)_2}). Then
$$
|r(z_k)| \leq q_2^2(z_0)|r(z_0)|< |r(z_0)| < \frac{p}{p-1}
(\sqrt{2p-1}-1).
$$
So, by Lemma \ref{lem:r(E(z))_r(z)} with $u = r(z_0)$, $z_{k+1} =
E(z_k)$ exists, $z_{k+1} \neq 0$ and
\begin{align*}
|r(z_{k+1})| & = |\phi(r(z_k))| \\
& \leq \sup_{m \geq 3}
\left\{\frac{|S_m(r(z_0))|}{|r(z_0)|^3}\right\}\cdot \frac{|r(z_0)|
+|\phi(r(z_0))|}{\big||r(z_0)| -
|\phi(r(z_0))|\big|} \cdot |r(z_k)|^3 \\
& \leq \sup_{m \geq 3}
\left\{\frac{|S_m(r(z_0))|}{|r(z_0)|^3}\right\}\cdot \frac{|r(z_0)|
+|\phi(r(z_0))|}{\big||r(z_0)| - |\phi(r(z_0))|\big|}
\left[q_2^{3^k-1}(z_0)\right]^3\cdot|r(z_0)|^3 \\
& = \sup_{m \geq 3}
\left\{\frac{|S_m(r(z_0))|}{|r(z_0)|}\right\}\cdot \frac{|r(z_0)|
+|\phi(r(z_0))|}{\big||r(z_0)| - |\phi(r(z_0))|\big|} \cdot
[q_2(z_0)]^{3^{k+1}-3} \cdot
|r(z_0)| \\
& = [q_2(z_0)]^{3^{k+1}-1} \cdot |r(z_0)|,
\end{align*}
which shows (\ref{ineq:abs_r(zk)_2}) by induction. The proof is
completed.
\end{proof}



Now, based on the above lemmas, we can obtain the following
convergence results for scalar Euler's method (\ref{it:EM}). Define
\begin{equation}
\label{set:R1} \MCR_1 := \big\{\lambda \in \CS: r(z_0,\lambda) \in
\overline{\MCD}_1 \text{ for some } z_0 \in
\CS\backslash\{0\}\big\},
\end{equation}
and
\begin{equation}
\label{set:R2} \MCR_2 := \left\{\lambda \in \CS: r(z_0,\lambda) \in
\MCD_0 \bigcap \MCD_2 \text{ for some } z_0 \in
\CS\backslash\{0\}\right\},
\end{equation}
where $\overline{\MCD}_1$ is the closure of $\MCD_1$ defined in
(\ref{set:D1}), $\mathcal {D}_0$ and $\mathcal {D}_2$ are defined in
(\ref{set:D0}) and (\ref{set:D2}), respectively.



\begin{lemma}
\label{lem:ScalarEMCon1} For any $\lambda \in \MCR_1 \bigcup
\MCR_2$, where $\MCR_1$ and $\MCR_2$ are defined by $(\ref{set:R1})$
and $(\ref{set:R2})$, respectively, the sequence $\{z_k(\lambda)\}$
generated by scalar Euler iteration $(\ref{it:EM})$ with some $z_0
\in \CS\backslash\{0\}$ for solving $(\ref{fun:f(z)})$ converges to
the principal $p$th root $\lambda^{1/p}$. Moreover, if $\lambda \neq
0$, then the convergence order is $3$.
\end{lemma}


\begin{proof}
We prove this lemma by four steps as follows.

Step 1. Suppose $\MCR_c$ is any closed domain in $\MCR_1$ or
$\MCR_2$ and that $0 \not\in \MCR_c$. We will prove in this step
$\{z_k(\lambda)\}$ converges uniformly to $z(\lambda)$, a $p$th root
of each $\lambda \in \MCR_c$, and that $z(\lambda)$ exists for each
$\lambda \in \MCR_1\bigcup\MCR_2$.

We write $r(z_k,\lambda) \triangleq r(z_k(\lambda),\lambda)$ for
short below. Since $ \sum\limits_{j=3}^\infty c_j r^j(z_0,\lambda)$
is analytic for each $\lambda \in \MCR_1\bigcup\MCR_2$ and $\MCR_c
\subset \MCR_1\bigcup\MCR_2$ is bounded, there is a
$\widehat{\lambda} \in
\partial\MCR_c$ such that
\begin{equation}
\label{eq:max_mod_1} \left|\sum_{j=3}^\infty c_j
r^j(z_0,\widehat{\lambda})\right| = \max_{\lambda \in
\MCR_c}\left|\sum_{j=3}^\infty c_j r^j(z_0,\lambda)\right|
\end{equation}
by the theorem of maximum modulus of analytic function. Let
$$
q(z_0) := \left\{
\begin{array}{ll}
\displaystyle \max_{\lambda \in \MCR_c}q_1(z_0,\lambda), & \mbox{if
$\MCR_c
\subset \MCR_1$,} \\
\displaystyle \max_{\lambda \in \MCR_c}q_2(z_0,\lambda), & \mbox{if
$\MCR_c \subset \MCR_2$},
\end{array}
\right.
$$
where $q_1(z_0,\lambda)$ and $q_2(z_0,\lambda)$ are defined in
(\ref{cons:q1(z0)}) and (\ref{cons:q2(z0)}), respectively. Then
$$
q(z_0) = \left\{
\begin{array}{ll}
q_1(z_0,\widehat{\lambda}) < 1, & \mbox{if $\MCR_c
\subset \MCR_1$,} \\
q_2(z_0,\widehat{\lambda}) < 1, & \mbox{if $\MCR_c \subset \MCR_2$},
\end{array}
\right.
$$
and
\begin{equation}
\label{ineq:abs_r(zk)} |r(z_k, \lambda)| \leq \left\{
\begin{array}{ll}
q^{3^{k-1}}(z_0), & \mbox{if $\MCR_c
\subset \MCR_1$,} \\
q^{3^k-1}(z_0) \cdot r_*, & \mbox{if $\MCR_c \subset \MCR_2$},
\end{array} \ \ \ k = 1,2,\ldots, \lambda \in \MCR_c
\right.
\end{equation}
by (\ref{eq:max_mod_1}), Lemmas \ref{lem:r(z)_convergence1} and
\ref{lem:r(z)_convergence2}, where $r_* := \max\limits_{\lambda \in
\MCR_c}|r(z_0,\lambda)|$ is a positive real independent on $\lambda
\in \MCR_c$. It follows $\{r(z_k, \lambda)\}$ converges uniformly to
0 with order 3 as $k \to \infty$ for all $\lambda \in \MCR_c$ and
that identities
\begin{equation}
\label{eq:zkp} z_k^p = \frac{\lambda}{1 - r(z_k,\lambda)}, \ \ \
\lambda \in \MCR_c, \ k = 0, 1, \ldots
\end{equation}
give the sequence $\{z_k(\lambda)\}$ is bounded uniformly for all
$\lambda \in \MCR_c$. Thus, there is a $M > 0$, independent on $k$
and $\lambda \in \MCR_c$, such that
\begin{equation}
\label{ineq:abs_bounded_M} \frac{1}{2p^2} |z_k(\lambda)| |2p + (p -
1)r(z_k, \lambda)| \leq M, \ \ k \geq 0, \lambda \in \MCR_c.
\end{equation}
By (\ref{it:EM_fun}) and (\ref{ineq:abs_bounded_M}),
\begin{align*}
|z_{k + 1}(\lambda) - z_k(\lambda)| & = \left|
\frac{1}{2p^2}z_k(\lambda) \left[(3p - 1) - 2(2p
- 1)\lambda z_k^{-p}(\lambda) + (p - 1)\lambda^2 z_k^{-2p}(\lambda)\right]\right| \\
& = \left| \frac{1}{2p^2}z_k(\lambda) \left[(1 - p) + 2(2p - 1)
r(z_k, \lambda) + (p -
1) (1 - r(z_k, \lambda))^2\right]\right| \\
& = \left| \frac{1}{2p^2}z_k(\lambda) \left[ 2p r(z_k, \lambda) + (p
-
1)r(z_k, \lambda)^2\right] \right| \\
& = \frac{1}{2p^2} |z_k(\lambda)| |r(z_k, \lambda)| |2p + (p - 1)r(z_k, \lambda)| \\
& \leq M |r(z_k, \lambda)|, \ \ \ \lambda \in \MCR_c, \ k =0, 1,
\ldots,
\end{align*}
which and (\ref{ineq:abs_r(zk)}) conclude that $\{z_{k + 1}(\lambda)
- z_k(\lambda)\}$ is majoriant by a geometrical sequence which
converges to zero and independent on $\lambda \in \MCR_c$. So,
$\{z_k(\lambda)\}$ is a Cauchy sequence for each $\lambda \in
\MCR_c$ and there is $z(\lambda)$ defined on $\MCR_c$ such that
$z_k(\lambda)$ converges uniformly to $z(\lambda)$ for all $\lambda
\in \MCR_c$. Let $k \to \infty$ in (\ref{eq:zkp}), we have
$$
z^p(\lambda) = \lambda, \ \ \ \lambda \in \MCR_c.
$$
That is, $z(\lambda)$ is a $p$th root of $\lambda \in \MCR_c$.


Step 2. Since for any $\lambda \in \MCR_1 \bigcup \MCR_2$, there is
a closed domain of $\MCR_1$ or $\MCR_2$ such that $\lambda$ belongs
to it. By Step 1, $z(\lambda)$ exists for each $\lambda \in
\MCR_1\bigcup \MCR_2$. In this step, we will show $z(\lambda)$
obtained in Step 1 is analytic in $\Int(\MCR_1)$, the interior of
$\MCR_1$, and $\MCR_2$. Therefore, $z(\lambda)$ located in a
single-valued branch of root function in $\Int(\MCR_1)$ and
$\MCR_2$.

In fact, by the definition of $z_k(\lambda), \lambda \in
\MCR_1\bigcup\MCR_2$, we have $z_k(\lambda)$ is analytic in
$\Int(\MCR_1)$ or $\MCR_2$. Since $\{z_k(\lambda)\}$ converges
uniformly to $z(\lambda)$ by Step 1, $z_k(\lambda)$ is analytic in
$\Int(\MCR_1)$ and $\MCR_2$ by Weierstrass theorem, seperately.
Recall that $z(\lambda)$ is a $p$th root of $\lambda \in
\Int(\MCR_1)$ or $\MCR_2$, we get $z(\lambda)$ located in a
single-valued branch of root function for $\lambda \in \Int(\MCR_1)$
or $\lambda \in \MCR_2$, seperately.


Step 3. In this step, we will show that $z(\lambda) \to
z(\lambda_0)$ as $\lambda \to \lambda_0$ from the inner side of
$\MCR_1$, where $\lambda_0 \in \partial\MCR_1$ and $\lambda_0 \neq
0$.

It is clear that $|r(z_0, \lambda_0)| < 1$ for any $\lambda_0 \in
\partial\MCR_1$ and $\lambda_0 \neq 0$. Then, there is $\delta_0 >
0$ such that closed domain $\overline{O}(\lambda_0, \delta_0)
\bigcap \MCR_1$ does not contains 0 and 1, where
$\overline{O}(\lambda_0, \delta_0) := \{\lambda \in \CS: |\lambda -
\lambda_0| \leq \delta_0\}$. By Step 1, $\{z_k(\lambda)\}$ converges
uniformly to $z(\lambda)$ for any $\lambda$ in
$\overline{O}(\lambda_0, \delta_0) \bigcap \MCR_1$. So, for any
$\varepsilon > 0$, there exists $K > 0$ such that for all $k \geq K$
and $\lambda \in \overline{O}(\lambda_0, \delta_0) \bigcap \MCR_1$,
\begin{equation}
\label{ineq:abs_zk-z_1} |z_k(\lambda) - z(\lambda)| <
\frac{\varepsilon}{3}.
\end{equation}
Since $z_k(\lambda)$ is analytic in $\MCR_1$ derives $z_k(\lambda)$
is continuous in $\overline{O}(\lambda_0, \delta_0) \bigcap \MCR_1$,
there exists $0 < \delta_1 < \delta_0$ such that
$\overline{O}(\lambda_0, \delta_1) \subset \overline{O}(\lambda_0,
\delta_0)$ and
\begin{equation}
\label{ineq:abs_zk-z_2} |z_k(\lambda) - z_k(\lambda_0)| <
\frac{\varepsilon}{3}, \ \ \ \forall \ \lambda \in
\overline{O}(\lambda_0, \delta_1)\bigcap \MCR_1.
\end{equation}
Thus, for all $\lambda$ in $\overline{O}(\lambda_0, \delta_1)\bigcap
\MCR_1$, we have
$$
|z(\lambda) - z(\lambda_0)| \leq |z(\lambda) - z_k(\lambda)| +
|z_k(\lambda) - z_k(\lambda_0)| + |z_k(\lambda_0) - z(\lambda_0)| <
\frac{\varepsilon}{3} + \frac{\varepsilon}{3} +
\frac{\varepsilon}{3} = \varepsilon
$$
by (\ref{ineq:abs_zk-z_1}) and (\ref{ineq:abs_zk-z_2}). That is,
$z(\lambda)$ is continuous at $\lambda = \lambda_0$. The
arbitrariness of $\lambda_0$ completes the proof of Step 3.


Step 4. This is the last step of the proof.

By Step 2, $z(\lambda)$ is analytic in $\Int(\MCR_1)$ and it is
located in a single-valued branch of $p$th root function. Since
$z_k(1) \equiv 1$ for all $k$ implies $z(1) = 1$, we have that
$z(\lambda)$ located in the single-valued branch containing 1. That
is, $z(\lambda)$ is the principal $p$th root of each $\lambda$ in
$\Int(\MCR_1)$. Since $z(\lambda) \to z(\lambda_0)$ as $\lambda \to
\lambda_0 \ (\lambda \in \Int(\MCR_1))$ for each $\lambda_0 \in
\partial\MCR_1$ by Step 3, we get $z(\lambda_0)$ is also the principal $p$th
root of $\lambda_0 \in \partial\MCR_1$.

By now, we have proved that $z(\lambda)$ is the principal $p$th root
of each $\lambda \in \MCR_1$ except $\lambda = 0$. When $\lambda =
0$, we have by (\ref{it:EM}) that
$$
z_k(0) = \frac{2p^2 - 3p + 1}{2p^2} z_{k-1}(0) = \left(\frac{2p^2 -
3p + 1}{2p^2}\right)^k z_0, \ \ k = 0, 1, 2, \ldots.
$$
So, $\{z_k(0)\}$ converges to 0, the principal $p$th root of 0,
linearly. Therefore, $z(\lambda)$ is the principal $p$th root of
each $\lambda \in \MCR_1$.

By Step 2 again, $z(\lambda)$ is analytic and locates in a
single-valued branch of $p$th root function for each $\lambda \in
\MCR_2$. Now, we can see that, if we can show $\MCR_2$ contains some
part of $\partial\MCR_1$, then the single-valued branch of
$z(\lambda)$ for $\lambda \in \MCR_2$ is the same branch of
$z(\lambda)$ for $\lambda \in \MCR_1$, which deduces that
$z(\lambda)$ is the principle $p$th root of $\lambda \in \MCR_1
\bigcup\MCR_2$. So, what we need to do is to prove $\MCR_2$ contains
some part of $\partial\MCR_1$.

Set $\lambda_0 = z_0^p$ and define
$$
S_m(z_0,\lambda) := \sum\limits_{j=3}^m c_jr^{j-1}(z_0,\lambda), \ \
\ S_\infty(z_0,\lambda) := \sum\limits_{j=3}^\infty
c_jr^{j-1}(z_0,\lambda), \ \ \ m \geq 3, \lambda \in \partial\MCR_1.
$$
Clearly, $S_\infty(z_0,\lambda)$ is continuous with respect to
$\lambda$ on $\partial\MCR_1$ and
\begin{equation}
\label{ineq:S(z0,lambda0)} 0 = \left|S_\infty(z_0,\lambda_0)\right|
= \min_{\lambda \in
\partial\MCR_1}\left|S_\infty(z_0,\lambda)\right| \leq
\left|S_\infty(z_0,\lambda)\right| \leq 1, \ \ \ \forall \ \lambda
\in
\partial\MCR_1.
\end{equation}
Since $|S_m(z_0,\lambda)|<1$ holds for all $\lambda \in
\MCR_1\bigcup\MCR_2$, we can choose $M < 1$ be a real satisfying
$$
\sup_{m\geq3}\left|S_m(z_0,\lambda)\right| < M, \ \ \ \lambda \in
\partial\MCR_1.
$$
By (\ref{ineq:S(z0,lambda0)}), there is $\delta > 0$ such that once
$|\lambda - \lambda_0| < \delta$, then
$$
 \left|S_\infty(z_0,\lambda)\right| <
 \frac{\frac{1}{M}-1}{\frac{1}{M} + 1},
$$
or equivalently,
$$
q_2(z_0,\lambda) = \sup_{m\geq 3}|S_m(z_0,\lambda)|\cdot
\frac{1+|S_\infty(z_0,\lambda)|}{1-|S_\infty(z_0,\lambda)|} < M
\cdot \frac{1}{M} = 1.
$$
Therefore, $\MCR_2$ contains some part of $\partial\MCR_1$. The
proof is completed.
\end{proof}



Choosing $z_0 \equiv 1$ in $\MCR_1$ and $\MCR_2$ given by
(\ref{set:R1}) and (\ref{set:R2}), respectively, one has that $\MCR
= \MCR_1 \bigcup \MCR_2$. So, we obtain from Lemma
\ref{lem:ScalarEMCon1} the following corollary:

\begin{corollary}
\label{cor:zk_convergence1} For any $\lambda \in \MCR$ defined in
$(\ref{set:R})$, the sequence $\{z_k(\lambda)\}$ generated by scalar
Euler iteration $(\ref{it:EM})$ with $z_0 = 1$ for solving
$(\ref{fun:f(z)})$ converges to the principal $p$th root
$\lambda^{1/p}$. Moreover, if $\lambda \neq 0$, then the convergence
order is $3$.
\end{corollary}



Next, we consider the case of matrix form. For the matrix $A \in
\CS^{n \times n}$ defined in (\ref{eq:f(X)=0}), let
\begin{equation}
\label{fun:R(X)} R(X) := \I - A X^{-p}, \ \ \ p \geq 2
\end{equation}
for any nonsingular matrix $ X \in \CS^{n \times n}$.

\begin{lemma}
\label{lem:R(E(X))_R(X)} Let $X \in \CS^{n\times n}$ be a
nonsingular matrix commuting with $A$ and $R(X)$ be defined in
$(\ref{fun:R(X)})$. If the spectrum $\sigma(R(X)) \subset \mathcal
{D}_0$ for some nonsingular matrix $X \in \CS^{n \times n}$, where
$\mathcal {D}_0$ is defined in $(\ref{set:D0})$, then $E(X)$
generated by $(\ref{it:EM_fun})$ is nonsingular, commutes with $A$
and
\begin{equation}
\label{eq:r(E(X))} R(E(X)) = \I - \left[\I - \frac{1}{p} R(X) -
\frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)R^2(X)\right]^{-p}\cdot(\I - R(X)) = \phi(R(X)).
\end{equation}
\end{lemma}

\begin{proof}
Clearly, $E(X)$ given by (\ref{it:EM_fun}) exists for any
nonsingular matrix $X \in \CS^{n \times n}$. Since $\sigma(R(X))
\subset \mathcal {D}_0\backslash\{0\}$, we get
\begin{align*}
\rho\left(\frac{1}{p} R(X) + \frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)R^2(X)\right) & \leq \frac{1}{p}\cdot\rho(R(X))
+
\frac{1}{2}\left(\frac{1}{p} - \frac{1}{p^2}\right)\rho^2(R(X)) \\
& = \frac{1}{p} \cdot
\rho(R(X)) \cdot \left(1 + \frac{p-1}{2p}\rho(R(X))\right) \\
& < \frac{1}{p} \cdot \frac{p}{p-1} \left(\sqrt{2p-1} - 1\right)
\cdot \left(1 + \frac{\sqrt{2p-1} - 1}{2} \right) \\
& = 1.
\end{align*}
Here, the first inequality follows from $\frac{1}{p} R(X) +
\frac{1}{2}(\frac{1}{p} - \frac{1}{p^2})R^2(X)$ is a polynomial of
the matrix $R(X)$. So
$$
\I - \frac{1}{p} R(X) - \frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)R^2(X)
$$
is nonsingular by Neumann Lemma. It follows that
\begin{align*}
E(X) & = \frac{1}{2p^2} X \left[(2 p^2 - 3 p + 1)\I + 2(2 p - 1)
AX^{-p} - (p - 1)(AX)^2\right] \\
& = X \left[\frac{2p^2 - 3p + 1}{2p^2}\I + \frac{2p -
1}{p^2}(\I - R(X)) - \frac{p - 1}{2p^2}(1 - R(X))^2\right] \\
& = X \left[\I - \frac{1}{p} R(X) - \frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)R^2(X)\right]
\end{align*}
is also nonsingular and commutes with $A$, and
\begin{equation*}
R(E(X)) = \I - \left[\I - \frac{1}{p} R(X) -
\frac{1}{2}\left(\frac{1}{p} -
\frac{1}{p^2}\right)R^2(X)\right]^{-p}\cdot(\I - R(X)) = \phi(R(X))
\end{equation*}
by the assumption of $X$ commutes with $A$. This completes the
proof.
\end{proof}



Thanks to Lemma \ref{lem:R(E(X))_R(X)}, we have

\begin{corollary}
\label{cor:E(Xk)} If $X_0 \in \CS^{n \times n}$ commutes with $A$
and  the spectrum $\sigma(R(X_0)) \subset \mathcal {D}_0$, where
$\mathcal {D}_0$ is defined in $(\ref{set:D0})$, then the sequence
$\{X_k\}$ starting from $X_0$ generated by Euler's method
$(\ref{it:EM})$ for solving $(\ref{eq:f(X)=0})$ exists.
\end{corollary}



\begin{lemma}
\label{lem:norm_R(E(X))_R(X)} If the spectrum $\sigma(R(X)) \subset
\MCD_1\backslash\{0\}$ for some nonsingular matrix $X \in \CS^{n
\times n}$, where $R(X)$  be defined in $(\ref{fun:R(X)})$ and
$\MCD_1$ is defined in $(\ref{set:D1})$, then, there is a
sub-multiplicative matrix norm $\|\cdot\|$ such that $\|R(X)\| \leq
1$ and
\begin{equation}
\label{ineq:norm_R(E(X))_1} \|R(E(X))\| \leq
\frac{\phi(\|R(X)\|)}{\|R(X)\|^3} \cdot \|R(X)\|^3 < \|R(X)\|^3,
\end{equation}
where $\phi$ is defined by $(\ref{fun:phi(z)})$.
\end{lemma}

\begin{proof}
It follows from Lemma \ref{lem:R(E(X))_R(X)} that $R(E(X))$ exists.
Since the spectrum $\sigma(R(X)) \subset \mathcal {D}_1$, the
spectral radius of $R(X)$ is less than 1. So, there is a
sub-multiplicative matrix norm $\|\cdot\|$ such that $\|R(X)\| < 1$.
Note that, for any $u \in (0,1)$,  It follows from Lemma
\ref{lem:RatFunTayExp} that
$$
\frac{\phi(u)}{u^3} = \sum_{j=3}^\infty c_j u^{j-3} <
\sum_{j=3}^\infty c_j = 1.
$$
Thus, this together with (\ref{eq:r(E(X))}) gives that
\begin{equation*}
\|R(E(X))\|  = \|\phi(R(X))\| \leq \phi(\|R(X)\|) =
\frac{\phi(\|R(X)\|)}{\|R(X)\|^3}\|R(X)\|^3 < \|R(X)\|^3,
\end{equation*}
which shows (\ref{ineq:norm_R(E(X))_1}). The proof is completed.
\end{proof}



\begin{lemma}
\label{lem:R(X)_convergence1} Let $R(X)$  be defined in
$(\ref{fun:R(X)})$. Suppose the spectrum $\sigma(R(X_0)) \subset
\mathcal {D}_1\backslash\{0\}$ for some nonsingular matrix $X_0 \in
\CS^{n \times n}$ and that $\|R(X_0)\| < 1$ for a sub-multiplicative
matrix norm $\|\cdot\|$, where $\mathcal {D}_1$ is defined in
$(\ref{set:D1})$. Let $\{X_k\}$ be the sequence  starting from $X_0$
generated by Euler's method $(\ref{it:EM})$ for solving
$(\ref{eq:f(X)=0})$. Then we have
\begin{equation}
\label{ineq:norm_R(Xk)_1} \|R(X_k)\| \leq q^{3^k-1}(X_0) \cdot
\|R(X_0)\|, \ \ \ k = 1, 2, \ldots,
\end{equation}
where
\begin{equation}
\label{cons:q(X0)} q(X_0) :=
\sqrt{\frac{\phi(\|R(X_0)\|)}{\|R(X_0)\|}} < 1
\end{equation}
and $\phi$ is defined by $(\ref{fun:phi(z)})$.
\end{lemma}

\begin{proof}
For $X_0$ chosen, by (\ref{ineq:norm_R(E(X))_1}) in Lemma
\ref{lem:norm_R(E(X))_R(X)}, we have
$$
\|R(X_1)\| \leq \frac{\phi(\|R(X_0)\|)}{\|R(X_0)\|^3} \|R(X_0)\|^3 =
q^2(X_0) \cdot \|R(X_0)\|.
$$
If (\ref{ineq:norm_R(Xk)_1}) holds for some $k \geq 1$, then by
Lemma \ref{lem:norm_R(E(X))_R(X)} again, one has that
\begin{align*}
\|R(X_{k+1})\| & \leq
\frac{\phi(\|R(X_k)\|)}{\|R(X_k)\|^3}\|R(X_k)\|^3\\
& \leq \frac{\phi(\|R(X_0)\|)}{\|R(X_0)\|^3}
\left[q^{3^k-1}(X_0)\right]^3 \cdot \|R(X_0)\|^3\\
& = [q(X_0)]^{3^{k+1}-1}\cdot\|R(X_0)\|.
\end{align*}
Thus, by induction, (\ref{ineq:norm_R(Xk)_1}) holds for all $k \geq
1$. This completes the proof.
\end{proof}



\begin{lemma}
\label{lem:R(X)_convergence2} Let $R(X)$  be defined in
$(\ref{fun:R(X)})$. If the spectrum $\sigma(R(X_0)) \subset \mathcal
{D}_2 \bigcap \mathcal {D}_0\backslash\{0\}$ for some nonsingular
matrix $X_0 \in \CS^{n \times n}$, where $\mathcal {D}_0$ and
$\mathcal {D}_2$ are defined in $(\ref{set:D0})$ and
$(\ref{set:D2})$, respectively. Let $\{X_k\}$ be the sequence
starting from $X_0$ generated by Euler's method $(\ref{it:EM})$ for
solving $(\ref{eq:f(X)=0})$. Then, there exists $\widehat{N} > 0$
such that
\begin{equation}
\label{ineq:norm_R(Xk)_2} \|R(X_k)\| \leq
[q(X_{\widehat{N}})]^{3^{k-\widehat{N}}-1} \cdot
\|R(X_{\widehat{N}})\|, \ \ \ \forall \ k
> \widehat{N},
\end{equation}
where $q(X_{\widehat{N}})$ is defined as $q(X_0)$ in
$(\ref{cons:q(X0)})$ by substituting $X_{\widehat{N}}$ for $X_0$.
\end{lemma}

\begin{proof}
For any $r(z_0) \in \sigma(R(X_0))$, let $\{z_k\}$ be the sequence
 generated by the scalar form of (\ref{it:EM}) starting from $z_0$. It
follows from Lemma \ref{lem:r(z)_convergence2} that there exists $N
> 0$ such that $|r(z_N)| < 1$. Then, we can get from Lemma
\ref{lem:r(z)_convergence1} that
\begin{equation*}
\label{ineq:abs_r(zk)_3} |r(z_k)| < |r(z_N)|^{3^{k-N}}, \ \ \
\forall \ k > N.
\end{equation*}
Define
$$
\widehat{N} := \max_{r(z_0) \in \sigma(R(X_0))} \{N: \text{choose a
}N
> 0 \text{ such that } |r(z_N)| < 1\}.
$$
Then, there exists a sub-multiplicative matrix norm $\|\cdot\|$ such
that $\|X_{\widehat{N}}\| < 1$. Thus, (\ref{ineq:norm_R(Xk)_2})
follows from Lemma \ref{lem:R(X)_convergence1}. This completes the
proof.
\end{proof}



\subsection{Proof of Theorem \ref{th:MatrixEMCon}}



The following lemma is taken from \cite[Theorem 4.15]{Higham2008},
which allows us to deduce convergence of the matrix iteration
sequence generated by Euler's method (\ref{it:EM}).

\begin{lemma}[\cite{Higham2008}]
\label{lem:MatIteConLem} Suppose that $g(x,t)$ is a rational
function with respect to its two variables and that $x^* =
f(\lambda)$ is an attracting fixed point of the iteration $x_{k + 1}
= g(x_k, \lambda), x_0 = \phi_0(\lambda)$, where $\phi_0$ is a
rational function and $\lambda \in \CS$. Then, the matrix sequence
generated by $X_{k + 1} = g(X_k, J(\lambda)), X_0 =
\phi_0(J(\lambda))$, converges to a matrix $X^*$ with $(X^*)_{ii}
\equiv f(\lambda)$, $i = 1, 2, \ldots, m$, where $J(\lambda) \in
\CS^{m \times m}$ is a Jordan block.
\end{lemma}


Now, we can prove Theorem \ref{th:MatrixEMCon} by applying the above
lemma together with the lemmas proposed in Section 3.1.



\begin{proof}[The proof of Theorem $\ref{th:MatrixEMCon}$]
By Corollary \ref{cor:E(Xk)}, the matrix sequence $\{X_k\}$
generated by Euler's method (\ref{it:EM}) starting from $X_0 = \I$
is well defined when the eigenvalues of $A$ are in $\mathcal {R}
\subset \mathcal {D}_0$. Thanks to Lemma \ref{lem:MatIteConLem},
$\{X_k\}$ converges to the principal $p$th root of $A$ follows from
Corollary \ref{cor:zk_convergence1}. The first part of theorem is
completed.

For the second part, let $X_* = A^{1/p}$ and $E_k = X_k - X_*$ for
$k \geq 0$. Due to the commutativity of $X_k$ and $X_*$, we have
\begin{align}
R(X_k) & = \I - AX_k^{-p} = (X_k^p - X_*^p)X_k^{-p} \nonumber\\
& = (X_k - X_*)\left(X_k^{p - 1} + X_k^{p - 2}X_* + \cdots +
X_k X_*^{p - 2} + X_*^{p - 1}\right)X_k^{-p} \nonumber\\
& = E_k \left(X_k^{p - 1} + X_k^{p - 2}X_* + \cdots + X_k X_*^{p -
2} + X_*^{p - 1}\right) X_k^{-p}, \ \ \forall \ k \geq 0
\label{eq:R(Xk)}
\end{align}
Set
$$
Y_k := \sum_{i = 1}^p X_k^{p - i} X_*^{i - 1} = X_k^{p - 1} + X_k^{p
- 2}X_* + \cdots + X_k X_*^{p - 2} + X_*^{p - 1}.
$$
Since $X_k$ converges to $A^{1/p}$ and all the eigenvalues of
$A^{1/p}$ are not in $\RS^-$, there exists nonnegative integer $N
> 0$ such that the eigenvalues $X_k$ are not in $\RS^-$ for all $k
\geq N$. Thus, the eigenvalues of $Y_k$ are also not in $\RS^-$ and
so $Y_k$ is nonsingular for $k \geq N$. Then, it follows from
(\ref{eq:R(Xk)}) that
\begin{equation}
\label{eq:Ek+1} E_{k + 1} = R(X_{k + 1}) X_{k + 1}^p Y_{k + 1}^{-1},
\ \ k \geq N.
\end{equation}
Thanks to (\ref{ineq:norm_R(Xk)_1}) and (\ref{ineq:norm_R(Xk)_2}) in
Lemmas \ref{lem:R(X)_convergence1} and \ref{lem:R(X)_convergence2},
respectively, there exists  $K_0 > 0$ such that $\|R(X_{k + 1})\| <
\|R(X_{k})\|^3$ for any $k \geq K_0$. It follows from
(\ref{eq:R(Xk)}) and (\ref{eq:Ek+1}) that
\begin{align}
\|E_{k + 1}\| & \leq \|R(X_{k + 1})\| \|X_{k + 1}\|^p \|Y_{k + 1}^{-1}\| \nonumber\\
& <  \|R(X_k)\|^3 \|X_{k + 1}\|^p \|Y_{k + 1}^{-1}\| \nonumber\\
& \leq \left(\|X_k^{-1}\|^p\|X_{k + 1}\|^p \|Y_k\|\|Y_{k +
1}^{-1}\|\right) \|E_k\|^3, \ \ \forall \ k \geq K_0.
\label{ineq_norm_Ek+1}
\end{align}
Thus $\{X_k\}$ is convergent guarantees that $\|X_k^{-1}\|^p\|X_{k +
1}\|^p \|Y_k\|\|Y_{k + 1}^{-1}\|$ is bounded for all $k \geq K_0$.
(\ref{ineq_norm_Ek+1}) concludes that the local convergence order is
3. This completes the proof.
\end{proof}




\section{A Preconditioned Schur Modification}


Let's begin with considering the stability of Euler iteration
(\ref{it:EM}). The analysis approach here follows the line of
stability analysis for Newton iteration in \cite{Smith2003}.

Let $A$ be nonsingular and diagonalizable with $n$ different
eigenvalues $\lambda_1, \ldots, \lambda_n$. Then, there exists a
nonsingular matrix $Z$ such that
$$
\inv{Z} A Z = \Lambda := \diag(\lambda_1, \ldots, \lambda_n).
$$
Suppose that the sequence $\{X_k\}$ generated by Euler iteration
(\ref{it:EM}) starting from $X_0 = \I$ converges to the principle
$p$th root $A^{1/p}$. Set $D_k := \inv{Z} X_k Z, \ k \geq 0$. It
follows from (\ref{it:EM}) that
$$
D_{k + 1} = \frac{1}{2p^2} D_k \left[(2p^2 - 3p + 1)\I + 2(2p -
1)\Lambda D_k^{-p} - (p - 1)\left(\Lambda D_k^{-p}\right)^2\right],
\ \ k \geq 0.
$$
Since $D_0 = \inv{Z} X_0 Z = \I$, $D_k$ is diagonal for any $k \geq
1$. Thus, if we set $D_k := \diag(d_1^{(k)}, \ldots, d_n^{(k)})$,
then $d_j^{(k)}$ converges to $\lambda_j^{1/p}$ as $k \to \infty$.
Let $\{\widetilde{X}_k\}$ be the sequence of computed iterates, then
\begin{align}
\widetilde{X}_{k + 1} & = \frac{1}{2p^2} \left((2p^2 - 3p +
1)\widetilde{X}_k + 2(2p - 1) A\widetilde{X}_k^{1-p} - (p - 1)A^2
\widetilde{X}_k^{1 -
2p}\right) \nonumber \\
& = \frac{2p^2 - 3p + 1}{2p^2}(X_k + \Delta_k) + \frac{2p - 1}{p^2}
A (X_k + \Delta_k)^{1 - p} - \frac{p - 1}{2p^2}A^2 (X_k +
\Delta_k)^{1 - 2p}.\label{eq:wide_Xk+1}
\end{align}
Set $\Delta_k = \widetilde{X}_k - X_k$ and $\widetilde{\Delta}_k :=
(\widetilde{\delta}_{ij}^{(k)}) = \inv{Z} \Delta_k Z$. Applying the
following expansion \cite{Smith2003}
\begin{align*}
(X + \Delta_X)^{1 - p} = X^{1-p} - \sum_{\ell = 1}^{p - 1} X^{\ell -
p} \Delta_X X^{-\ell} + O(\|\Delta_X\|^2)
\end{align*}
to (\ref{eq:wide_Xk+1}), where $\|\cdot\|$ denotes the 2-norm, one
has that
\begin{align*}
\widetilde{X}_{k + 1} & = \frac{2p^2 - 3p + 1}{2p^2}(X_k + \Delta_k)
+ \frac{2p - 1}{p^2} A \left[X_k^{1-p} - \sum_{\ell = 1}^{p - 1}
X_k^{\ell
- p} \Delta_k X_k^{-\ell}\right] \\
& \qquad - \frac{p - 1}{2p^2}A^2 \left[X_k^{1-2p} - \sum_{\ell =
1}^{2p - 1} X_k^{\ell - 2p} \Delta_k X_k^{-\ell}\right] +
O(\|\Delta_k\|^2), \ \ k \geq 0.
\end{align*}
We have for each $k \geq 0$,
\begin{align*}
\Delta_{k + 1} & = \widetilde{X}_{k + 1} - X_{k + 1} \nonumber\\
& = \frac{2p^2 - 3p + 1}{2p^2} \Delta_k - \frac{2p - 1}{p^2} A
\sum_{\ell = 1}^{p - 1} X_k^{\ell - p} \Delta_k X_k^{-\ell} +
\frac{p - 1}{2p^2} A^2 \sum_{\ell = 1}^{2p - 1} X_k^{\ell - 2p}
\Delta_k X_k^{-\ell},% \label{eq:Deltak+1}
\end{align*}
and
\begin{align*}
\widetilde{\Delta}_{k + 1} & = \frac{2p^2 - 3p + 1}{2p^2}
\widetilde{\Delta}_k - \frac{2p - 1}{p^2} \Lambda \sum_{\ell = 1}^{p
-
1} D_k^{\ell - p} \widetilde{\Delta}_k D_k^{-\ell} \\
& \qquad + \frac{p - 1}{2p^2} \Lambda^2 \sum_{\ell = 1}^{2p - 1}
D_k^{\ell - 2p} \widetilde{\Delta}_k D_k^{-\ell} +
O(\|\Delta_k\|^2).
\end{align*}
This gives
\begin{align}
\label{eq:wide_deltak+1}\widetilde{\delta}_{ij}^{(k + 1)}  & =
\widetilde{\delta}_{ij}^{(k)} \pi_{ij}^{(k)} + O(\|\Delta_k\|^2), \
\ \ i, j = 1,2,\ldots,n, \ k \geq 0,
\end{align}
where
$$
\pi_{ij}^{(k)} = \frac{2p^2 - 3p + 1}{2p^2}
 - \frac{2p - 1}{p^2} \lambda_i \sum_{\ell
= 1}^{p - 1} \frac{1}{(d_i^{(k)})^{p - \ell} (d_j^{(k)})^\ell}  +
\frac{p - 1}{2p^2} \lambda^2_i \sum_{\ell = 1}^{2p - 1}
\frac{1}{(d_i^{(k)})^{2p - \ell} (d_j^{(k)})^\ell}.
$$
 Since $d_j^{(k)}$ converges to $\lambda_j^{1/p}$ as $k \to
\infty$ for each $j = 1, \ldots, n$, we may write $d_j^{(k)} =
\lambda_j^{1/p} + \varepsilon_j^{(k)}, j = 1,2,\ldots,n, \ k \geq
0$, where $\varepsilon_j^{(k)} \to 0$ as $k \to \infty$. Hence, for
each $i, j = 1,2,\ldots,n, \ k = 0,1,\ldots,$
\begin{equation}
\label{eq:pi_ij_k} \pi_{ij}^{(k)} = \frac{2p^2 - 3p + 1}{2p^2} -
\frac{2p - 1}{p^2} \sum_{j = 1}^{p - 1}
\left(\frac{\lambda_i}{\lambda_j}\right)^{j/p} + \frac{p - 1}{2p^2}
\sum_{j = 1}^{2p - 1} \left(\frac{\lambda_i}{\lambda_j}\right)^{j/p}
+ O(\varepsilon^{(k)}).
\end{equation}
where $\varepsilon^{(k)} := \max\limits_i|\varepsilon^{(k)}_i|$ and
the constant interfere with ``$O$" is independent on $i, j = 1, 2,
\ldots, n$.

To guarantee the numerical stability of Euler iteration
(\ref{it:EM}), we should require from (\ref{eq:wide_deltak+1}) and
(\ref{eq:pi_ij_k}) that
\begin{equation}
\label{neq:pi(k)neq1} \left|\frac{2p^2 - 3p + 1}{2p^2} - \frac{2p -
1}{p^2} \sum_{j = 1}^{p - 1}
\left(\frac{\lambda_i}{\lambda_j}\right)^{j/p} + \frac{p - 1}{2p^2}
\sum_{j = 1}^{2p - 1}
\left(\frac{\lambda_i}{\lambda_j}\right)^{j/p}\right| \leq 1, \ \ i,
j = 1,\ldots, n.
\end{equation}
Obviously, this is a very restrictive condition on $A$, even for
that $A$ is Hermitian positive definite. For example, in the case $p
= 2$, (\ref{neq:pi(k)neq1}) becomes
$$
-3 \leq - 5 \kappa_2(A)^{1/2} + \kappa_2(A) + \kappa_2(A)^{3/2} \leq
5,
$$
which is equivalent to $\kappa_2(A) \leq 5$, where $\kappa_2(A) :=
\|A\|_2 \|\inv{A}\|_2$.

The above defects of Euler iteration (\ref{it:EM}) should be
weakened. One way often used is to modify Euler iteration
(\ref{it:EM}) into the following coupled version by introducing the
auxiliary matrix $N_k (k\geq 0)$:
\begin{equation}
\label{it:CoupledEM} \left\{
\begin{array}{l}
\displaystyle X_{k + 1} = X_k \left(\frac{(2 p^2 - 3p + 1)\I + 2(2p
- 1)N_k - (p - 1)N_k^2}{2p^2}\right), \ \ X_0 =
\I, \\
\displaystyle N_{k + 1} = \left(\frac{(2 p^2 - 3p + 1)\I + 2(2p -
1)N_k - (p - 1)N_k^2}{2p^2}\right)^{- p} N_k, \ \ N_0 = A.
\end{array} \right.
\end{equation}
Clearly, $N_k = AX_k^{-p}$ and $\{X_k\}$ generated by
(\ref{it:CoupledEM}) is same as the sequence of Euler method
(\ref{it:EM}). We call it coupled Euler's iteration. When the
sequence $\{X_k\}$ generated by (\ref{it:CoupledEM}) converges to
$A^{1/p}$, $N_k$ converges to $\I$. The computational cost of
iteration (\ref{it:CoupledEM}) is $2(4 + \vartheta \log_2p)n^3$
flops per step for some $\vartheta \in [1,2]$ by means of the binary
powering technique \cite[Algorithm 11.2.2]{Golub1996}.


Note that, while we purely use coupled Euler iteration
(\ref{it:CoupledEM}) to compute $A^{1/p}$,  bad numerical results
will still appear. A simple example (see TEST 1 in Section 4)
illustrates this observation.


In order to avoid poor numerical results, we make Schur
decomposition before we begin to use coupled Euler iteration
(\ref{it:CoupledEM}), similar to Algorithms 3 and 4 in
\cite{Iannazzo2008}, based on the idea of Algorithm 3.3 in
\cite{GuoHigham2006}. New algorithm is given as follows.

\begin{algorithm}
\floatname{algorithm}{算法}
\caption{Schur-Euler algorithm using
(\ref{it:CoupledEM}) for computing $A^{1/p}$} \label{al:SE} Given $A
\in \CS^{n \times n}$ with no nonpositive real eigenvalues, an
integer $p = 2^{k_0}q$ with $k_0 \geq 0$ and $q$ odd. This algorithm
computes $A^{1/p}$ via a Schur decomposition and Euler iteration.
%\newcounter{newlist}
\begin{list}{\arabic{newlist}.}{\usecounter{newlist}
\setlength{\rightmargin}{0em}\setlength{\leftmargin}{1.2em}}
\item
Compute the Schur decomposition of $A = QRQ^*$;
\item
If $q = 1$, then $k_1 = k_0$; else choose the smallest $k_1 \geq
k_0$ such that for each eigenvalue $\lambda$ of $A$,
$\lambda^{1/2^{k_1}} \in \MCR_{\text{E}}$ defined in
(\ref{set:R_pra});
\item
Compute $B = R^{1/2^{k_1}}$ by taking the square root $k_1$ times;
if $q = 1$, then set $X = QB\tran{Q}$; else continue;
\item
Compute $C = B^{1/q}$ by using the coupled Euler iteration
(\ref{it:CoupledEM}) and set $X = Q C^{2^{k_1 - k_0}} \tran{Q}$.
\end{list}
\end{algorithm}


Recall that, one square root costs $n^3/3$ flops and the evaluation
of (\ref{it:CoupledEM}) costs $2(4 + \vartheta \log_2p)n^3$ flops,
where $\vartheta \in [1,2]$. Thus, $25n^3$ flops for the real Schur
decomposition plus $k_1$ times square root, $k_1 - k_0$ times matrix
multiplications and $4n^3$ flops to form $X$, the total
computational cost of Algorithm \ref{al:SE} is about
$$
\left(29 + \frac{k_1}{3} + 2 (k_1 - k_0) + 2k_2(4 + \vartheta
\log_2p)\right) n^3 \ \text{flops}, \ \ \vartheta \in [1,2],
$$
where we assume that $k_2$ iterations of (\ref{it:CoupledEM}) are
done exactly. Numerical experiments in Section 4 show that,
Algorithm \ref{al:SE} has a well numerical behavior. Moreover, in
most cases, Algorithm \ref{al:SE} has less computational time and
also can save some square roots in preprocessing.






\section{Numerical Experiments}



The goal of numerical experiments in this section is to illustrate
that Euler's method is as good as Newton method and Halley method
for computing the principal $p$th root of a matrix. To support this
claim, we now test Algorithm \ref{al:SE} and compare its
computational performance with existing three algorithms presented
in \cite{GuoHigham2006,Iannazzo2008} on computational error,
computational time and the number of iterations.

Upto 6 Tests are performed here. Test 1 is to show that it may
suffers from bad numerical results when we purely use coupled Euler
iteration (\ref{it:CoupledEM}) to compute $A^{1/p}$ even though the
matrix $A$ is simple. Tests 2-6 used by many authors
\cite{GuoHigham2006,Guo2010,Iannazzo2006,Iannazzo2008,HighamLin2011}
are to compare numerical behavior with other algorithms. The results
show that, in most cases, Algorithm \ref{al:SE} requires less
computational time and less steps of iterations. Meanwhile, it has
more numerical accuracy of the computed solution than others
algorithms.

Our numerical experiments were carried out in MATLAB 7.0 running on
a PC Intel Pentium P6200 of 2.13 GHz CPU. To measure of the quality
of a computed solution $X$, we use the relative residual $\rho_A(X)$
and relative error $\textup{err}(X)$ as follows:
\begin{equation}
\label{eq:RelResErr} \rho_A(X) = \frac{\|A - X^p\|}{\|X\| \|\sum_{j
= 0}^{p - 1} \tran{(X^{p - 1 - j})}\otimes X^j\|}, \ \ \
\textup{err}(X) = \frac{\|A - X^p\|}{\|A\|},
\end{equation}
where $\otimes$ denotes the Kronecker product and $\|\cdot\|$
denotes the Frobenius norm. Note that the relative residual
$\rho_A(X)$ (given in \cite{GuoHigham2006}) is more practically
useful definition of relative residual (e.g., for testing purposes)
than relative error and that the averaged CPU time computed by the
standard MATLAB function \textsf{cputime}. The averaged time was
computed by repeating 100 times for each test matrix. Moreover, we
use `iter' to stand for the number of the iterations.


For simplicity, in the following tests, we denote
\begin{enumerate}
\item
SE: Schur-Euler algorithm, Algorithm \ref{al:SE};
\item
PSN: parameter Schur-Newton algorithm from \cite[Algorithm
3.3]{GuoHigham2006};
\item
SN: Schur-Newton algorithm from \cite[Algorithm 3]{Iannazzo2008};
\item
SH: Schur-Halley algorithm from \cite[Algorithm 4]{Iannazzo2008}.
\end{enumerate}
The iterations in the above four algorithms are stopped when $\|N_k
- \I\| < \sqrt{n}u/2$, where $n$ is the size of $A$ and $u = 2^{-52}
\approx2.2204\me-16$.


TEST 1.  We first give a simple example to illustrate that it
usually suffers from bad numerical results when we purely use
coupled Euler iteration (\ref{it:CoupledEM}) to compute $A^{1/p}$.
Consider the following simple $3 \times 3$ matrix
$$
A = \left[\begin{array}{ccc} 1 & 1/2 & 0 \\
1/2 & 2 & 1/2 \\
0 & 1/2 & 3 \end{array}\right],
$$
and compute the $p$th root $A^{1/p}$ for $p = 2:15$. In Figure
\ref{fig:relerr_CEM}, we give the relative error $\textup{err}(X)$
for each $p$. As one can see, for $p = 2:6$ the coupled Euler
iteration (\ref{it:CoupledEM}) gives good relative error (Figure
\ref{fig:relerr_CEM} (a)), but the errors deteriorate as $p = 7:15$
(Figure \ref{fig:relerr_CEM} (b)). Algorithm \ref{al:SE} (taking
some preprocessing before using oupled Euler iteration
(\ref{it:CoupledEM})) shows good numerical stability (Figure
\ref{fig:relerr_CEM} (c)).

\begin{figure}[h]
\centering
\subfigure[]{\includegraphics[width=0.32\textwidth]{fig_relerr_coupledEM_1.eps}}
\subfigure[]{\includegraphics[width=0.32\textwidth]{fig_relerr_coupledEM_2.eps}}
\subfigure[]{\includegraphics[width=0.32\textwidth]{fig_relerr_coupledEM_Schur_3.eps}}
\caption{(a) and (b) show the relative error for the $p$th root
$A^{1/p}$ by using coupled Euler iteration (\ref{it:CoupledEM}); (c)
is the result of Algorithm \ref{al:SE}.}\label{fig:relerr_CEM}
\end{figure}

TEST 2. In this test, we compare the relative error on computing the
principal $p$th root of the matrices \cite{HighamLin2011} (depend on
variable $\varepsilon$)
\begin{equation}
\label{mat:Ae}
A(\varepsilon) = \left[\begin{array}{cc} 1 & 1 \\
0 & 1 + \varepsilon \end{array}\right]
\end{equation}
for $p = 12, 15, 30$. We choose $\varepsilon = 10^{-t}$ with 65
equally spaced values of $t \in [0,16]$. As is pointed out in
\cite{HighamLin2011}, $A(\varepsilon)$ approaches a defective matrix
as $t$ increases. The relative errors for the four algorithms are
shown in Figure \ref{fig:relerr_Ae}. The numerical results of
Algorithm \ref{al:SE} are very good. Note that, there exists 9
values of $t$ cannot be computed by using PSN while $t$ approaches
to 16. We also computed the relative residuals of these four
algorithms on matrix $A(\varepsilon)$, the results were broadly
similar to those shown in Figure \ref{fig:relerr_Ae}.

\begin{figure}[h]
\centering
\subfigure[$p=12$]{\includegraphics[width=0.32\textwidth]{fig_relerr_se_spn_sn_sh_p12.eps}}
\subfigure[$p=15$]{\includegraphics[width=0.32\textwidth]{fig_relerr_se_spn_sn_sh_p15.eps}}
\subfigure[$p=30$]{\includegraphics[width=0.32\textwidth]{fig_relerr_se_spn_sn_sh_p30.eps}}
\caption{The relative errors for the four algorithms on matrix
(\ref{mat:Ae}).}\label{fig:relerr_Ae}
\end{figure}


TEST 3. To compare the computational time of the existing three
algorithms with Algorithm \ref{al:SE}, we select the classical test
matrix (Frank matrix) from the MATLAB \textsf{gallery} function with
$15 \times 15$ matrix which no nonpositive real eigenvalues and
compute the principal $p$th root for $10 \leq p \leq 300$. Note
that, the Frank matrix is an upper Hessenbery with determinant 1.
This test matrix was also used in
\cite{GuoHigham2006,GrecoIannazzo2010,Iannazzo2006}. The eigenvalues
of a Frank matrix are positive and occur in reciprocal pairs, half
of which are ill-conditioned. The averaged CPU time for any value of
$p$ is shown in Fingure \ref{fig:cputime}. As one can see in Figure
\ref{fig:cputime}, the required CPU time of Algorithm \ref{al:SE} is
generally less than other algorithms, especially, PSN.

\begin{figure}[h]
\centering
\subfigure[]{\includegraphics[width=0.42\textwidth]{fig_cputime_psn.eps}}
\subfigure[]{\includegraphics[width=0.42\textwidth]{fig_cputime_sn.eps}} \\
\subfigure[]{\includegraphics[width=0.42\textwidth]{fig_cputime_sh.eps}}
\subfigure[]{\includegraphics[width=0.42\textwidth]{fig_cputime_se.eps}}
\caption{The CPU time (in seconds) required by using the four
Algorithms to compute the principal $p$th root of $15 \times 15$
Frank matrix for $p = 10:300$.}\label{fig:cputime}
\end{figure}


TEST 4. Consider the following two matrices which take from
\cite{Guo2010} and \cite{Iannazzo2008}, respectively.
$$
S_1 = \left[\begin{array}{rrrr} 0.44 & - 0.88 & - 0.38 & - 0.50 \\
0.68 & 2.15 & 0.48 & 0.11 \\
0.61 & 0.77 & 2.14 & 1.04 \\
- 0.16 & - 0.30 & - 0.67 & 1.33
\end{array}\right], \ \ \
S_2 = \left[\begin{array}{ccc} - 1 & - 2 & 2 \\
- 4 & - 6 & 6 \\
- 4 & - 16 & 13 \end{array}\right].
$$
Let $A_1 = S_1^5$ and $A_2 = S_2^{15}$. The eigenvalues of $A_1$ are
$15.2477, 0.2724 \pm 16.0066 \,\textup{i}, 1.1030$ and of $A_2$ are
$1, 2, 3$. We now compute $A_1^{1/5}$ and $A_2^{1/15}$ using the
four algorithms. The computational results are given in Table
\ref{tab:test3}. The relative error is computed by $\|X - S\|/\|S\|$
and the relative residuals is computed by using
(\ref{eq:RelResErr}). As is shown in Table \ref{tab:test3},
Algorithm \ref{al:SE} has a less pronounced advantage for computing
$A_1^{1/5}$ and $A_2^{1/15}$.

\begin{table}[h]
\begin{center}
\begin{tabular}{cc@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c}
\hline
& Algorithm & CPU time (s)& iter & $k_1$ & $\rho(X)$ & err($X$) \\
\hline
$A_1^{1/5}$ \\
& PSN & 3.91e-03 & 5 & 2 & 1.75e-15 & 2.05e-15 \\
& SN & 3.28e-03 & 6 & 2 & 4.67e-16 & 1.05e-15 \\
& SH & 3.13e-03 & 3 & 2 & 6.15e-16 & 9.63e-16 \\
& SE & 3.59e-03 & 3 & 3 & 8.77e-16 & 1.55e-15 \\
$A_2^{1/15}$ \\
& PSN & 6.09e-03 & 5 & 5 & 1.48e-15 & 2.67e-08 \\
& SN & 5.17e-03 & 6 & 4 & 5.74e-15 & 2.67e-08 \\
& SH & 5.00e-03 & 3 & 4 & 7.82e-15 & 2.67e-08 \\
& SE & 4.53e-03 & 5 & 5 & 2.25e-14 & 2.67e-08 \\

\hline
\end{tabular}
\end{center}
\caption{Results for computing $A_1^{1/5}$ and $A_2^{1/15}$ by using
the four Algorithms. } \label{tab:test3}
\end{table}

TEST 5. Consider a random nonnormal $8 \times 8$ matrix constructed
as $A = Q R \tran{Q}$, where $Q$ is a random orthogonal matrix and
$R$ is in the Schur form with eigenvalues $\alpha_j \pm \mathrm{i}
\beta_j, \alpha_j = - j^2/10, \beta_j = - j, j = 1:n/2$ and elements
$(2j, 2j + 1)$ equal to $- 450$. This example was used in
\cite{GuoHigham2006} and \cite{Iannazzo2008} to compare the behavior
of Algorithms PSN, SN and SH. We use the four algorithms to compute
the $p$th of this random matrix and list in Table \ref{tab:test4}
the results in terms of CPU time, number of iterations, relative
residual and relative error. Algorithm \ref{al:SE} shows a good
accuracy and requires less computational time.

\begin{table}[h]
\begin{center}
\begin{tabular}{cc@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c}
\hline
& Algorithm & CPU time (s)& iter & $k_1$ & $\rho(X)$ & err($X$) \\
\hline
$p = 5$ \\
& PSN & 2.34e-03 & 5 & 3 & 8.68e-15 & 3.11e-12 \\
& SN & 2.97e-03 & 5 & 2 & 8.62e-15 & 3.08e-12 \\
& SH & 3.13e-03 & 3 & 2 & 8.54e-15 & 3.06e-12 \\
& SE & 1.41e-03 & 3 & 2 & 8.52e-15 & 3.05e-12 \\
$p = 7$ \\
& PSN & 5.00e-03 & 5 & 3 & 1.45e-14 & 3.83e-12 \\
& SN & 4.22e-03 & 4 & 2 & 1.50e-14 & 3.96e-12 \\
& SH & 4.38e-03 & 3 & 2 & 1.55e-14 & 4.07e-12 \\
& SE & 4.06e-03 & 4 & 2 & 1.47e-14 & 3.87e-12 \\
\hline
\end{tabular}
\end{center}
\caption{Results for a random nonnormal matrix by using the four
Algorithms. } \label{tab:test4}
\end{table}


TEST 6. We select the classical test matrices (prolate matrix and
Frank matrix) with no nonpositive real eigenvalues from the MATLAB
\textsf{gallery} function together with Hilbert matrix, and then
compute their principal $p$th root for $p = 18, 33, 81$ by using the
four algorithms. These test matrices also were used in
\cite{Iannazzo2006,GrecoIannazzo2010,GuoHigham2006}. Note that, the
prolate matrix is a symmetric ill-conditioned Toeplitz matrix whose
elements are $A_{ii} = 1/2, A_{ij} = \sin(\pi(j-i)/2)/(\pi(j - i))$.
The Hilbert matrix is a notable example of an ill-conditioned
matrix. The elements of the Hilbert matrix are $H_{ij} = 1/(i + j -
1)$. The results of the comparison are summarized in Table
\ref{tab:relres_err_it}. As we can see that, in most cases,
Algorithm \ref{al:SE} has less computational time and smaller errors
than others algorithms. It also confirms that Euler's method is a
very good choice for computing $p$th roots of a matrix.

\begin{landscape}
\begin{table}
\begin{center}
%\begin{tabular}{cc|c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c@{\hspace{0.7cm}}c}
\begin{tabular}{cccccccccccccccccc}
\hline
& \multicolumn{4}{c}{$p = 18$} &&& \multicolumn{4}{c}{$p = 33$} &&& \multicolumn{4}{c}{$p = 81$}\\
\cline{2-6} \cline{8-12} \cline{14-18}
& CPU time & iter & $k_1$ & $\rho(X)$ & err($X$)&& CPU time & iter & $k_1$ & $\rho(X)$ & err($X$) && CPU time & iter & $k_1$ & $\rho(X)$ & err($X$)\\
\hline
 \\
 \multicolumn{6}{l}{Hilbert matrix ($7 \times 7$)} \\
 \\
PSN & 1.06e-02 & 5 & 5 & 1.31e-14 & 6.47e-14 & & 1.06e-02 & 5 & 5 & 3.77e-14 & 2.12e-13 && 1.05e-02 & 5 & 5 & 6.77e-14 & 4.30e-13\\
SN & 8.44e-03 & 6 & 4 & 6.09e-15 & 3.01e-14 & & 8.59e-03 & 6 & 4 & 1.65e-14 & 9.31e-14 && 8.59e-02 & 6 & 4 & 2.63e-14 & 1.67e-13\\
SH & 8.59e-03 & 4 & 4 & 1.34e-15 & 6.60e-15 & & 8.59e-03 & 4 & 4 & 2.43e-14 & 1.37e-13 && 8.59e-03 & 4 & 4 & 2.51e-14 & 1.60e-13\\
SE & 7.03e-03 & 5 & 3 & 3.19e-15 & 1.57e-14 & & 8.44e-03 & 4 & 4 & 4.53e-15 & 2.55e-14 && 8.44e-03 & 4 & 4 & 2.45e-14 & 1.56e-13\\
 \\
\multicolumn{6}{l}{Prolate matrix ($10 \times 10$)}\\
 \\
PSN & 2.16e-02 & 5 & 5 & 3.96e-15 & 3.45e-14 & & 2.03e-02 & 5 & 5 & 1.33e-14 & 1.03e-13 && 2.31e-02 & 4 & 5 & 3.79e-14 & 3.64e-13\\
SN & 1.81e-02 & 6 & 4 & 1.29e-15 & 1.12e-14 & & 1.81e-02 & 6 & 4 & 5.74e-15 & 5.26e-14 && 1.77e-02 & 6 & 4 & 9.05e-15 & 8.70e-14\\
SH & 1.72e-02 & 4 & 4 & 1.84e-15 & 1.61e-14 & & 1.59e-02 & 4 & 4 & 1.03e-14 & 9.47e-14 && 1.81e-02 & 4 & 4 & 2.55e-14 & 2.45e-13\\
SE & 1.23e-02 & 4 & 3 & 1.29e-15 & 1.13e-14 & & 1.33e-02 & 4 & 3 & 3.41e-15 & 3.13e-14 && 1.25e-02 & 4 & 3 & 1.08e-14 & 1.04e-13\\
 \\
\multicolumn{6}{l}{Frank matrix ($12 \times 12$)}\\
 \\
PSN & 2.31e-02 & 5 & 4 & 1.46e-15 & 1.54e-08 & & 2.39e-02 & 5 & 4 & 7.65e-15 & 2.51e-08 && 2.44e-02 & 5 & 4 & 1.17e-13 & 6.58e-08\\
SN & 1.69e-02 & 6 & 3 & 1.46e-15 & 1.55e-08 & & 1.67e-02 & 6 & 3 & 5.06e-15 & 1.66e-08 && 1.70e-02 & 6 & 3 & 1.15e-13 & 6.45e-08\\
SH & 1.77e-02 & 4 & 3 & 1.52e-15 & 1.61e-08 & & 1.67e-02 & 4 & 3 & 8.09e-15 & 2.65e-08 && 1.80e-02 & 4 & 3 & 8.24e-14 & 4.73e-08\\
SE & 1.63e-02 & 4 & 3 & 1.17e-15 & 1.24e-08 & & 1.80e-02 & 4 & 3 & 7.45e-15 & 2.44e-08 && 1.78e-02 & 4 & 3 & 1.07e-13 & 6.00e-08\\
\hline
\end{tabular}
\end{center}
\caption{Results for computing principal $p$th root of $7 \times 7$
Hilbert matrix, $10 \times 10$ Prolate matrix and $12 \times 12$
Frank matrix by using the four Algorithms. }
\label{tab:relres_err_it}
\end{table}
\end{landscape}












\section{关于~Halley~法的注记}

While the iterative form of Halley's method for computing $A^{1/p}$
is given by:
\begin{equation}
\label{it:HM} X_{k+1} = X_k\left[(p+1)X_k^p + (p-1)A\right]^{-1}
\left[(p-1)X_k^p + (p+1)A\right],\ \ \ k=0,1,2,\ldots,
\end{equation}
provided $X_0$ commutes with $A$ and that $(p+1)X_k^p + (p-1)A$ is
nonsingular for any $k \geq 0$.


令
\begin{equation}
\label{fun:phi(z)} \phi_\nu(z) := 1 - u^p(z)(1 - z), \ \ \  z \in
\MCD_{0,\nu}, \ \nu = 1,2,
\end{equation}
其中
\begin{equation*}
u_2(z) := \frac{1-\frac{p-1}{2p}z}{1 - \frac{p+1}{2p}z}, z \in
\MCD_{0,2},
\end{equation*}

\begin{equation}
\label{set:D0} \MCD_{0,2} := \left\{z \in \CS: |z| < \frac{2p}{p +
1}\right\}.
\end{equation}


Define
\begin{equation}
\label{set:R} \MCR_\nu := \left\{z\in \CS: 1 - z \in
\overline{\MCD}_1 \bigcup \left(\MCD_{0,\nu} \bigcap
\MCD_{2,\nu}\right)\right\}
\end{equation}
and
\begin{equation}
\label{set:R_hat} \widehat{\MCR}_\nu := \left\{z\in\CS: 1-z \in
\mathcal {D}_1 \bigcup \left(\mathcal {D}_{0,\nu} \bigcap \mathcal
{D}_{2,\nu}\right)\right\}, \ \ \ \nu = 1,2,
\end{equation}
where $\MCD_{0,\nu}$ are defined in (\ref{set:D0}),
$\overline{\MCD}_1$ is the closure of $\MCD_1$ defined by
\begin{equation}
\label{set:D1} \MCD_1  := \{z \in \CS: |z| < 1\},
\end{equation}
and
\begin{equation}
\label{set:D2} \left\{
\begin{array}{l}
\displaystyle \MCD_{2,1}  := \left\{z \in \CS: \sup_{m \geq 2}
\left\{\frac{|S_{1,m}(z)|}{|z|}\right\} \cdot \frac{|z|
+|\phi_1(z)|}{\big||z| - |\phi_1(z)|\big|} < 1 \right\},\\
\displaystyle \MCD_{2,2}  := \left\{z \in \CS: \sup_{m \geq 3}
\left\{\frac{|S_{2,m}(z)|}{|z|}\right\} \cdot \frac{|z|
+|\phi_2(z)|}{\big||z| - |\phi_2(z)|\big|} < 1 \right\},
\end{array}
\right.
\end{equation}
where
\begin{equation}
\label{ser:Sm(z)} \left\{
\begin{array}{ll}
\displaystyle S_{1,m}(z) = \sum\limits_{j = 2}^{m} c_{1,j} z^{j},
& z \in \MCD_{0,1},\\
\displaystyle S_{2,m}(z) = \sum\limits_{j = 3}^{m} c_{2,j} z^{j}, &
z \in \MCD_{0,2},
\end{array}
\right.
\end{equation}
$c_{\nu,j} = \phi^{(j)}_\nu(0)/j!$ and $\phi_\nu$ are defined in
(\ref{fun:phi(z)}), $\nu = 1,2$.  As for $\phi_2(z)$, the similar
result has been proved in \cite{Lin2010}.






Similar to Newton's method (\ref{it:NM}), we can obtain the
following theorem on convergence and convergence order for Halley's
method (\ref{it:HM}).

\begin{theorem}
\label{th:MatrixHMCon} If all eigenvalues of $A \in \CS^{n \times
n}$ are in $\MCR_2$ defined by $(\ref{set:R})$ and all zero
eigenvalues of $A$ are semisimple, then the matrix sequence
$\{X_k\}$ generated by Halley's method $(\ref{it:HM})$ starting from
$X_0 = \I$ converges to the principal $p$th root $A^{1/p}$.
Moreover, if all eigenvalues are in
$\widehat{\MCR}_2\backslash\{0\}$ defined by $(\ref{set:R_hat})$,
then the convergence is cubic.
\end{theorem}







\begin{figure}[t!]
\centering
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p25m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p25m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p25m500.eps}}\\
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p100m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p100m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p100m500.eps}}\\
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p400m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p400m100.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p400m500.eps}}\\
\caption{The approximating regions of $\MCR_2$ defined in
(\ref{set:R}) for $p = 25, 100, 400$ and $m = 20, 100, 500$, where
the red and blue regions denote the sets $\{z\in\CS: 1-z \in
\overline{\MCD}_1\}$ and $\{z\in \CS: 1-z \in \MCD_{0,2} \bigcap
\MCD_{2,2}\}$, respectively. }\label{fig:HM_ConvReg}
\end{figure}


\begin{remark}
Similar to Newton's method, in Figure \ref{fig:HM_ConvReg}, we plot
the approximating region $\MCR_2$ defined in (\ref{set:R}) with nine
cases in the complex plane for Halley's method, where the red and
blue regions denote the sets $\{z \in \CS: 1-z \in
\overline{\MCD}_1\}$ and $\{z\in \CS: 1-z \in
\MCD_{0,2}\bigcap\MCD_{2,2}\}$, respectively. We can see from Figure
\ref{fig:HM_ConvReg} that, for a fixed $p$, approximating regions
(cf. (a)-(c) for $p=25$, (d)-(f) for $p=100$ and (g)-(i) for
$p=400$) are almost the same when $m=20, 100, 500$, respectively. To
this end, it suffices to choose $m=20$ in practical numerical
computation. Furthermore, we find that, for a fixed $m(\geq20)$, the
approximating regions $\MCR_2$ with various $p$ are also almost the
same.
\end{remark}





\begin{remark}
It is also inconvenient, similar to the case of $\MCR_1$ defined in
(\ref{set:R}) for Newton's method, to check whether a eigenvalue
$\lambda$ belongs to $\MCR_2$ defined in (\ref{set:R}) in practice.
We can also define a feasible region which is acceptable
approximation to $\MCR_2$ and allows us to determine easily whether
a eigenvalue belongs to it. Define
\begin{equation}
\label{set:R_H_pra} \MCR_2^{\text{H}} = \MCD_3 \bigcup \MCD_5,
\end{equation}
where $\MCD_3$ is defined in (\ref{set:D3}) and
\begin{equation*}
\MCD_5 := \left\{z\in\CS: |\arg(z)|<\frac{\pi}{3},
|1-z|<\frac{7}{5}\right\}.
\end{equation*}
The actual convergence region $\MCR_2$ and the new feasible region
$\MCR_2^{\text{H}}$ are depicted in Figure \ref{fig:HM_PraConvReg}
with $p=7, 20, 100$ and fixed $m=20$, where the yellow parts denote
the region $\MCR_2^{\text{H}}$.
\end{remark}

\begin{figure}[t!]
\centering
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_PraConvReg_p7m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_PraConvReg_p20m20.eps}}
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_PraConvReg_p100m20.eps}}\\
\caption{For fixed $m=20$ and $p=7,20,100$, the actual convergence
regions $\MCR_2$ defined in (\ref{set:R}) (the union of the red and
blue parts) and the approximate convergence regions
$\MCR_2^{\text{H}}$ defined in (\ref{set:R_H_pra}) (the yellow
parts).}\label{fig:HM_PraConvReg}
\end{figure}




\begin{remark}
It is worth noting that, the convergence regions $\MCR_2$ defined in
(\ref{set:R}) for Halley's method (\ref{it:HM}) is more useful than
the one given by Iannazzo in \cite[Algorithm 4]{Iannazzo2008} in
which the choice of the region (the disk of center 8/5 and radius 1)
is heuristic and is based on the observation the experimental
regions of convergence. The comparison of the regions $\MCR_2$,
$\MCR_2^{\text{H}}$ and the disk of center 8/5 and radius 1 is shown
in Figure \ref{fig:HM_PraConvRegVSIannazzo}.
\end{remark}


\begin{figure}[t!]
\centering
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_ConvReg_p100m20_Iannazzo.eps}}
\quad\quad\quad\quad
\subfigure[]{\includegraphics[width=0.3\textwidth]{fig_HM_PraConvReg.eps}}\\
\caption{(a) The convergence region $\MCR_2$ defined in
(\ref{set:R}) with $p=100$ and $m=20$ (the union of the red and blue
parts) and the the disk of center 8/5 and radius 1 given by Iannazzo
(the yellow parts); (b) the feasible region $\MCR_2^{\text{H}}$
defined in (\ref{set:R_H_pra}) (the red part) and the disk of center
8/5 and radius 1 (the blue
part).}\label{fig:HM_PraConvRegVSIannazzo}
\end{figure}
